<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Solo travelers</title>
    <link href="css/research_objectives.css" rel="stylesheet" />
  </head>
  <body>
    <div class="container">
      <h1>Technologies Used</h1>

      <ul class="custom-list">
        <li>
            <strong>1. <em>Python</em></strong><br>
            <span class="description">Description: Python was used for backend development and machine learning model integration. Its powerful libraries (such as OpenCV, NumPy, and TensorFlow) were employed for image processing and training custom deep learning models to detect speech impairments from facial features. Python's simplicity and extensive ecosystem made it ideal for handling AI tasks efficiently.</span>
        </li>

        <li>
            <strong>2. <em>Java</em></strong><br>
            <span class="description">Description: Java was used for developing the Android-based mobile application. Known for its reliability and portability across devices, Java enabled the creation of a robust client-side interface that supports real-time user interaction, image upload, and API integration for speech therapy functionalities.</span>
        </li>

        <li>
            <strong>3. <em>FireBase</em></strong><br>
            <span class="description">Description: Firebase served as the backend-as-a-service platform for user authentication, cloud storage, and real-time database functionalities. It helped log user sessions, store diagnostic results, and facilitate secure data communication between the mobile frontend and backend services. </span>

        <li>
            <strong>4. <em>YOLOv8</em></strong><br>
            <span class="description">Description: YOLOv8 (You Only Look Once) was used for object detection and facial analysis tasks in identifying cleft palate symptoms. Chosen for its speed and accuracy, the model was trained on annotated child facial datasets and integrated into the system for real-time inference.</span>
        </li>

        <li>
            <strong>5. <em>Roboflow</em></strong><br>
            <span class="description">Description: Roboflow was used to annotate, preprocess, and manage the custom dataset for training the object detection model. It streamlined image augmentation and helped export datasets in YOLO-compatible formats, boosting the modelâ€™s accuracy and generalization.</span>
        </li>

        <li>
            <strong>6. <em>Flask</em></strong><br>
            <span class="description">Description: Flask, a lightweight Python web framework, was used to build RESTful APIs that connected the trained AI models to the mobile application. It handled image processing requests, ran inference, and returned structured JSON responses in real time.</span>
        </li>

        <li>
            <strong>7. <em>Google Colab </em></strong><br>
            <span class="description">Description: Google Colab was utilized for training and evaluating machine learning models. With built-in support for GPUs, it allowed resource-intensive operations such as model tuning, augmentation testing, and real-time visualization using TensorBoard.</span>
        </li>

        <li>
            <strong>7. <em>Android Studio</em></strong><br>
            <span class="description">Description: Android Studio was the IDE used for developing and testing the mobile application. XML was used for UI layout design, and Java was used for implementing core functionality, API integration, and error handling mechanisms.</span>
        </li>
    </ul>
    </div>
  </body>
</html>
